services:
  postgres:
    image: postgres:16.4
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: warehouse
    ports:
      - "5433:5432"
    volumes:
      - ./docker/initdb:/docker-entrypoint-initdb.d
      - ./postgres-volume/storage/postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
  adminer:
    image: adminer
    environment:
      ADMINER_DEFAULT_SERVER: postgres
    ports:
      - "8080:8080"
  airflow:
    image: apache/airflow:2.9.3-python3.11
    ports: 
      - "8081:8080"
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./:/opt/bluealpha
      - ./airflow/dags:/opt/airflow/dags
    working_dir: /opt/bluealpha
    environment:
      PYTHONPATH: /opt/bluealpha
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgres:5432/warehouse
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      DBT_PROFILES_DIR: /opt/bluealpha/dbt_project
      WAREHOUSE_HOST: postgres
      WAREHOUSE_PORT: '5432'
      WAREHOUSE_DB: warehouse
      WAREHOUSE_USER: postgres
      WAREHOUSE_PASSWORD: postgres
    command: >
      bash -c "
      pip install --no-cache-dir -r /opt/bluealpha/requirements.txt &&
      airflow db migrate &&
      (airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true) &&
      (airflow scheduler &) && 
      exec airflow webserver
      "
